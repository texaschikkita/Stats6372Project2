suppressWarnings(rm(a,adStats,anovaDF,anovaMatrix,anovaTable,autocorrBarPlot,autocorrDigits,autocorrExport,autocorrLags,autocorrTable,autocorrValues,autoDF,cDigits,classStats,classTable,coeffdf,coeffDigits,coeffMatrix,coeffTable,coeffTableExcel,coeffStars,constantInd,cutoffValue,errorStats,errorStatsTable1,errorStatsTable2,expCoeff,foldNum,fValues,has,i,indVariables,logitAUC,logitRsquared,logitStats,mainTitle,maxError,minError,modelAdjRsquared,modelRsquared,modelAIC,modelCV,modelDescription,modelDev,modelDf,modelEquation,modelInfo,modelMAE,modelMAPE,modelMASE,modelName,modelNumber,modelRsqStdErr,modelSize,modelStdErr,modelsummarydf,modelSummaryDigits,modelSummaryTable,modelSummaryTableExport,MSdigits,nData,newColumn,nFolds,nIterations,nLags,nMissing,nonMissing,nTest,nTrain,nullModel,nVariables,outputExcel,regData,regModel,regScript,regStats,regY,residualDf,RMSEdigits,RMSEnull,RMSEnulltest,RMSEtest,RMSEtrain,roccurve,rowNumbers,stepInfo,SSdigits,SSregression,SSresidual,SStotal,stdCoeffValues,testClass,testClassTable,testData,testErrors,testMethod,testPctCor,testPred,testRMSEratio,testRsquared,testSet,testSetName,testStats,testStatsExport,testStatsValues,testTable,testTableExport,testTableOmitMV,testTitle,testTrueNeg,testTruePos,testValues,trainClass,trainData,trainErrors,trainPctCor,trainPred,trainSet,trainSetName,trainStats,trainStatsExport,trainStatsValues,trainTable,trainTableExport,trainTrueNeg,trainTruePos,tValue,variableSelection,vifValues,wants,x,yFitted,yMax,yMin,yName,yResids,yValues))writeLines(" -----Begin script GLOWdata.Model.3.04.29.22.23.44.r produced by RegressItLogistic version 2022.12.14 on DESKTOP-F9IAE9K at time 04.29.22.23.44 ")writeLines(" ")modelDescription <- "Logistic regression model Model.3 for FRACTURE in data frame GLOWdata " writeLines(modelDescription)variableSelection <- "Variable selection:  all " writeLines(variableSelection)testMethod <- "Out-of-sample test: random 1 test set size 0.2 seed 0 " writeLines(testMethod)regData <- GLOWdataregY <- GLOWdata$FRACTUREnData <- nrow(regData)regScript <- "GLOWdata.Model.3.04.29.22.23.44.r"yName <- "FRACTURE"nIterations <- 1rowNumbers <- 1:nData# ----- begin loop for multiple models fittedfor (i in 1:nIterations) {writeLines(" ")writeLines(paste("----------Model fitted on iteration ", i," of ", nIterations," ----------"))nTest <- round(0.2*nData,0)nTrain <- nData - nTesttrainSet <- sample(rowNumbers, nTrain)trainData <- regData[trainSet,]testData <- regData[-trainSet,]testSet <- rowNumbers[-trainSet]Model.3 <- glm(FRACTURE ~ 1, family = binomial, data = trainData)constantInd <- 1stepInfo <- "" regModel <- Model.3modelName <- "Model.3" trainPred <- predict(regModel, trainData, se.fit= T, type = "response", interval = "prediction", level = .95)trainErrors <- residuals(regModel, trainData, type = "response", interval = "prediction", level = .95)# ----- table of training-set actual and predicted values, residuals, deviance residuals and Pearson's residuals# ----- deviance is the square of the deviance residualtrainTable <- cbind(na.omit(cbind(trainSet, regY[trainSet],trainPred$fit,regY[trainSet]-trainPred$fit)),residuals(regModel,type="deviance"),residuals(regModel,type="pearson"),hatvalues(regModel),cooks.distance(regModel)) # ----- more variables are created for later referencenMissing <- nTrain - dim(trainTable)[1]nTrain <- dim(trainTable)[1]yValues <- trainTable[,2]yFitted <- trainTable[,3]RMSEtrain <- (var(trainErrors) * (nTrain - 1) / nTrain + mean(trainErrors) ^ 2) ^ 0.5RMSEdigits <- max(0, round(4 - log10(RMSEtrain)))   # number of significant digits to use for rounding some statistics in tablesRMSEnull <- (var(yValues)* (nTrain - 1) / nTrain) ^ 0.5 colnames(trainTable) <- c("Train Row#", "Actual", "Predicted", "Residual","Deviance.Res.", "Pearson.Res.", "Leverage", "Cooks D")nVariables <- regModel$rankmodelName <- paste(modelName,".rand.",i,".of.",nIterations,sep="")modelInfo <- paste(modelName,"for",yName,"(",nVariables - constantInd,"variable(s),",stepInfo,"n=",nTrain,")")# ----- build table of training set results for exporting to ExceltrainTableExport <- rbind(c("Train Row#", "Actual", "Predicted", "Residual", "Std.Res.", "AbsStdRes", "Leverage", "Cooks D"),trainTable)colnames(trainTableExport) <- rep("",8)testPred <- predict(regModel, testData, se.fit= T, type = "response", interval = "prediction", level = .95)testErrors <- regY[testSet] - testPred$fittestTable <- cbind(testSet, regY[testSet], testPred$fit, testPred$se.fit, testErrors)testErrors <- na.omit(testTable[,5])nTest <- length(testErrors)RMSEtest <- (var(testErrors) * (nTest - 1) / nTest + mean(testErrors) ^ 2) ^ 0.5testValues <- na.omit(as.matrix(testTable[, c(2,3)]))[,1]RMSEnulltest <- (var(testValues) * (nTest - 1) / nTest)^.5testRMSEratio <- RMSEtest/RMSEnulltesttestRsquared <- 1-testRMSEratio^2colnames(testTable) <- c("Test Row#", "Actual", "Predicted", "Std.Error.M","Error")# ----- build table of test set results for exporting to ExceltestTableExport <- rbind(c("Test Row#", "Actual", "Predicted", "Std.Error.M","Error"),testTable)residualDf <- regModel$df.residualtValue <- qt(0.975, residualDf)modelDf <- regModel$df.null - regModel$df.residualmodelDev <- regModel$null.deviance - regModel$deviancemodelRsquared <- 1 - regModel$deviance/regModel$null.deviancemodelAdjRsquared <- max(0,1 - (regModel$deviance + 2*(modelDf + 1))/regModel$null.deviance)modelMASE <- NAwriteLines(" ")writeLines(paste("Regression statistics:",modelName,"for",yName,"(#variables =",nVariables-1,")"))roccurve <- roc(yValues  ~ yFitted, data = trainData)logitStats  <- t(matrix(c(modelRsquared,modelAdjRsquared,regModel$df.residual,RMSEtrain,mean(yValues),nTrain,nMissing,roccurve$auc,summary.glm(regModel)$aic))) print(kable(logitStats, col.names = c("R-Sqr","Adj R-Sqr","Df","RMSE","Mean","#Fitted","#Missing","ROC area","AIC"),digits = c(3,3,0,4,4,0,0,3,3)))writeLines("") expCoeff <- exp(summary.glm(regModel)$coefficients[,1])# ----- calculate variance inflation factors if the model includes a constant and has more than one independent variablevifValues <- NA if(grepl("Int", variable.names(regModel)[1]) & nVariables > 1){   vifValues <- c(0,vif(regModel))   } coeffTable  <-  c(summary.glm(regModel)$coefficients[,c(1,2)],round(summary.glm(regModel)$coefficients[,c(3,4)],6),vifValues,expCoeff) coeffTable <- matrix(coeffTable,nrow=1,ncol=6)writeLines("Coefficient estimates:") cDigits <- max(pmax(0,round(3 - log10(as.numeric(coeffTable[,2])),0)))rownames(coeffTable) <- variable.names(regModel)coeffTable <-gsub("_","|",kable(coeffTable, col.names = c("Coeff","StdErr","z stat","P(>_z_)","VIF","ExpCoeff"),digits = c(cDigits,cDigits,3,3,1,3)))coeffTable <- gsub("NA","  ",coeffTable)colnames(coeffTable) <- NULLprint(coeffTable, row.names = F)writeLines(" ")# ----- build single-row table of error statistics in training period errorStats <- t(matrix(c(nTrain,mean(trainTable[,2]),RMSEtrain,RMSEnull,RMSEtrain/RMSEnull,RMSEtrain^2)))rownames(errorStats) <- yNameif(nTest>0){# ----- add 2nd row with error statistics in test period errorStats <- rbind(errorStats,c(nTest,mean(na.omit(testTable[,2])),RMSEtest,RMSEnulltest,RMSEtest/RMSEnulltest,RMSEtest^2))rownames(errorStats) <- c("Train","Test")}writeLines(" ")writeLines("Comparative error statistics:")print(kable(errorStats, col.names = c("#","Mean","ModelRMSE","ConstRMSE","Model/Const","MSE(BrierScore)"),digits= c(0,4,4,4,2,4)))writeLines(" ")if(nTest>0){writeLines("The test set constant model is based on the mean of the dependent variable in the test set.")}writeLines(" ")cutoffValue <- 0.5writeLines(" ")writeLines(paste("Classification accuracy with cutoff value = ",cutoffValue))writeLines("Training set: ")trainClass <- table(yValues, fitted(regModel) > cutoffValue)if(as.vector(dim(trainClass)[1])==1){if(rownames(trainClass)[1] == "FALSE") { trainClass <- rbind(trainClass,c(0,0)) } else {trainClass <- rbind(c(0,0),trainClass)}}if(as.vector(dim(trainClass)[2])==1){if(colnames(trainClass)[1] == "FALSE") { trainClass <- cbind(trainClass,c(0,0))} else {trainClass <- cbind(c(0,0),trainClass)}}rownames(trainClass ) <- c("Actual: 0","        1")colnames(trainClass ) <- c("Predicted: 0","   1")trainPctCor <- round((trainClass [1, 1] + trainClass [2, 2]) / sum(trainClass ), 4)trainTruePos <- round(trainClass [2, 2]/ sum(trainClass [2,]), 4)trainTrueNeg <- round(trainClass [1, 1]/sum(trainClass [1,]), 4)trainClass <- cbind(trainClass , Total = rowSums(trainClass ))trainClass <- rbind(trainClass , Total = colSums(trainClass ))trainStats <- trainClassrownames(trainStats) <- rep(" ",3)  colnames(trainStats) <- rep(" ",3) trainStatsExport <- rbind(c("Predict 0","Predict 1","Total","","","Predict 0","Predict 1","Total"),cbind(trainStats,matrix("",nrow=3,ncol=2),trainStats/trainStats[3,3]))trainStatsExport <- cbind(c("Training results","Actual 0","Actual 1","Total"),trainStatsExport)trainStats <- rbind(c("Predict 0","Predict 1","Total","","","Predict 0","Predict 1","Total"),cbind(trainStats,matrix("",nrow=3,ncol=2),round(trainStats/trainStats[3,3],3)))trainStats <- cbind(c("Training results","Actual 0","Actual 1","Total"),trainStats)trainStatsExport <- rbind(trainStatsExport,rep("",9),c("Fraction correct",trainPctCor,"","RMSE train","","","","",""))trainStatsExport <- rbind(trainStatsExport,c("True positive rate",trainTruePos,"","RMSE const","","","","",""))trainStatsExport <- rbind(trainStatsExport,c("True negative rate",trainTrueNeg,"","Cutoff value","","","","",""))trainStatsExport[6:8,5] <- c(RMSEtrain,RMSEnull,cutoffValue)trainStatsExport[1:4,6] <- c("Fraction","Actual 0","Actual 1","Total")trainStats[1:4,6] <- c("Fraction","Actual 0","Actual 1","Total")trainStatsValues <- trainStats[2:4,2:9]rownames(trainStatsValues) <- c("Actual 0","Actual 1","Total")print(kable(trainStatsValues,col.names=c("Predict 0","Predict 1","Total","","Fraction","Predict 0","Predict 1","Total")))writeLines(" ")trainStats <- rbind(c(paste("Training set classification accuracy with cutoff value = ",cutoffValue,":"),"","","","","","","",""),trainStats)trainStats <- rbind(rep("",9),trainStats)trainStatsExport <- rbind(c(paste("Training Set Classification Accuracy With Cutoff Value = ",cutoffValue,":",modelInfo),"","","","","","","",""),trainStatsExport)trainStatsExport <- rbind(rep("",9),trainStatsExport)writeLines(" ")writeLines("Fraction tested = 0.2 at random ")writeLines(" ")if(nTest>0){testClass <- table(regY[testSet], testPred$fit > cutoffValue)if(as.vector(dim(testClass)[1])==1){if(rownames(testClass)[1] == "FALSE") { testClass <- rbind(testClass,c(0,0)) } else {testClass <- rbind(c(0,0),testClass)}}if(as.vector(dim(testClass)[2])==1){if(colnames(testClass)[1] == "FALSE") { testClass <- cbind(testClass,c(0,0))} else {testClass <- cbind(c(0,0),testClass)}}if(dim(testClass)[1] > 1){rownames(testClass) <- c("Actual: 0","        1")}colnames(testClass) <- c("Predicted: 0","   1")testPctCor <- round((testClass[1, 1] + testClass[2, 2]) / sum(testClass), 4)testTruePos <- round(testClass[2, 2]/sum(testClass[2,]), 4)testTrueNeg <- round(testClass[1, 1]/sum(testClass[1,]), 4)testClass <- cbind(testClass, Total = rowSums(testClass))testClass <- rbind(testClass, Total = colSums(testClass))testStats <- testClassrownames(testStats) <- rep(" ",3)  colnames(testStats) <- rep(" ",3) testStatsExport <- rbind(c("Predict 0","Predict 1","Total","","","Predict 0","Predict 1","Total"),cbind(testStats,matrix("",nrow=3,ncol=2),testStats/testStats[3,3]))testStatsExport <- cbind(c("Test results","Actual 0","Actual 1","Total"),testStatsExport)testStats <- rbind(c("Predict 0","Predict 1","Total","","","Predict 0","Predict 1","Total"),cbind(testStats,matrix("",nrow=3,ncol=2),round(testStats/testStats[3,3],3)))testStats <- cbind(c("Test results","Actual 0","Actual 1","Total"),testStats)testStatsExport <- rbind(testStatsExport,rep("",9),c("Fraction correct",testPctCor,"","RMSE test","","","","",""))testStatsExport <- rbind(testStatsExport,c("True positive rate",testTruePos,"","RMSE const","","","","",""))testStatsExport <- rbind(testStatsExport,c("True negative rate",testTrueNeg,"","Cutoff value","","","","",""))testStatsExport[6:8,5] <- c(RMSEtest,RMSEnulltest,cutoffValue)testStatsExport[1:4,6] <- c("Fraction","Actual 0","Actual 1","Total")testStats[1:4,6] <- c("Fraction","Actual 0","Actual 1","Total")writeLines(paste("Test Set:"))testStatsValues <- testStats[2:4,2:9]rownames(testStatsValues) <- c("Actual 0","Actual 1","Total")print(kable(testStatsValues,col.names=c("Predict 0","Predict 1","Total","","Fraction","Predict 0","Predict 1","Total")))testStats <- rbind(c(paste("Test set classification accuracy with cutoff value = ",cutoffValue,":"),"","","","","","","",""),testStats)testStats <- rbind(rep("",9),testStats)testStatsExport <- rbind(c(paste("Test Set Classification Accuracy With Cutoff Value = ",cutoffValue,":",modelInfo),"","","","","","","",""),testStatsExport)testStatsExport <- rbind(rep("",9),testStatsExport)} else {testPctCor <- 0testTruePos <-0 testTrueNeg <- 0}classStats <- t(matrix(c(nTrain,trainPctCor,trainTruePos,trainTrueNeg)))rownames(classStats) <- yNameif(nTest>0){classStats <- rbind(classStats,c(nTest,testPctCor,testTruePos,testTrueNeg))rownames(classStats) <- c("Train","Test")}writeLines(" ")writeLines("Overall accuracy:")print(kable(classStats, col.names = c("#","Correct","True Pos","True Neg"),digits = c(0,rep(3,3))))writeLines(" ")testTitle = paste(": sample ",i," of ", nIterations)testSetName = "Test" trainSetName = paste("Train (iteration=" , i,")") mainTitle = paste(modelName," for ",yName,testTitle)modelSize <- paste("n=", nTrain, ", #var=",length(regModel$coefficients)-1,", ")layout(matrix(c(1,2,3,4),2,2))plot(regModel, main = mainTitle, cex.main = 0.95)layout(matrix(c(1,2,3),3,1))yMin <- min(na.omit(regY),yValues,yFitted, na.omit(testTable[,2]),na.omit(testTable[,3]))yMax <- max(na.omit(regY),yValues,yFitted, na.omit(testTable[,2]),na.omit(testTable[,3]))plot(c(trainTable[,1],testTable[,1]),c(yValues,testTable[,2]),main=mainTitle, type = "o", col = "black",lty=0, xlab = NA , ylab= yName, cex.main = 0.95, ylim = c(yMin,yMax))title(main = ( paste(modelSize, trainSetName, "RMSE=",round(RMSEtrain,RMSEdigits), ", ",testSetName," RMSE=", round(RMSEtest,RMSEdigits))),  line = 0.5, cex.main = 0.8)points(trainTable[,1],yFitted,pch = 16, col = "red",cex=1)points(testTable[,1],testTable[,3],pch = 17, col = "blue",lty=3,cex=1)barplot(yValues- yFitted,main=mainTitle,xlab = NA, ylab= "Residual", cex.main=0.95,ylim = c(-1,1))title(sub = paste(modelSize, trainSetName, "RMSE=",round(RMSEtrain,RMSEdigits), ", ",testSetName," RMSE=", round(RMSEtest,RMSEdigits)), cex.sub = 0.8)writeLines(" ")trainPred <- predict(regModel,type=c("response"))plot(roccurve, asp = NA,legacy.axes = TRUE,main = paste (mainTitle, " AUC = ", round(print(roccurve$auc),3)))if(i==1){ modelNumber <- 1if(nTest == 0){testPctCor <- 0testTruePos <-0 testTrueNeg <- 0 }modelSummaryTable <- rbind(modelNumber,nTrain,nVariables-constantInd,RMSEtrain,modelAdjRsquared,roccurve$auc,nTest, RMSEtest, cutoffValue,trainPctCor,trainTruePos,trainTrueNeg,testPctCor,testTruePos,testTrueNeg)modelSummaryDigits <- c(0,0,0,RMSEdigits,3,3,0,RMSEdigits,2,2,2,2,2,2,2)modelSummaryTableExport <- cbind(c("Iteration","#Train","#Variables","TrainRMSE","R-Sqr.","ROC area","#Test","TestRMSE","Cutoff","Correct","TruePos","TrueNeg","TestCorrect","TestTruePos","TestTrueNeg"),modelSummaryTable)rownames(modelSummaryTable) <- c("Iteration","#Train","#Variables","TrainRMSE","R-Sqr.","ROC area","#Test","TestRMSE","Cutoff","Correct","TruePos","TrueNeg","TestCorrect","TestTruePos","TestTrueNeg")} else { modelNumber <- modelNumber+1newColumn <- rbind(modelNumber, nTrain,nVariables-constantInd,RMSEtrain, modelAdjRsquared, roccurve$auc,nTest, RMSEtest, cutoffValue, trainPctCor, trainTruePos, trainTrueNeg, testPctCor, testTruePos, testTrueNeg)writeLines(" ")modelSummaryTableExport <- cbind(modelSummaryTableExport,newColumn)modelSummaryTable <- cbind(modelSummaryTable,newColumn)}  #  i=1modelEquation <- paste("Model.3 <- glm(",as.character(formula(Model.3)[2])," ~ ",paste(names(coef(Model.3)), collapse='+'),",family = binomial,data = GLOWdata)") modelEquation <- sub("(Intercept)+","",modelEquation,fixed=TRUE)}  # ----- end of loop for multiple modelswriteLines("------------------------------------------------------------")writeLines("")writeLines(modelDescription)writeLines(variableSelection)writeLines(testMethod)writeLines("Error statistics for all models fitted: ")colnames(modelSummaryTable) <- c(rep("",nIterations))modelSummaryTableExport <- cbind(modelSummaryTableExport,rowMeans(modelSummaryTable))modelSummaryTable <- cbind(modelSummaryTable,rowMeans(modelSummaryTable))modelSummaryTable[1,ncol(modelSummaryTable)] <- NA #  NA in lower left corner of statistics table is used as a numeric placeholder for the text "Average" to be substituted below" modelSummaryTable <- t(modelSummaryTable)errorStatsTable1 <-  gsub("     NA","Average",kable(modelSummaryTable[,1:min(8,ncol(modelSummaryTable))], digits = modelSummaryDigits[1:min(8,ncol(modelSummaryTable))], row.names = FALSE))print(errorStatsTable1)errorStatsTable2 <-  gsub("     NA","Average",kable(modelSummaryTable[,c(1:3,10:12,7,13:15)], digits = modelSummaryDigits[c(1:3,10:12,7,13:15)], row.names = FALSE))writeLines(" ")writeLines(paste("Classification accuracy with cutoff value = ",round(cutoffValue,2)))print(errorStatsTable2)modelSummaryTableExport[1,ncol(modelSummaryTableExport)] <- "Average"modelSummaryTableExport <- t(modelSummaryTableExport)nColumns  <- ncol(modelSummaryTableExport) modelSummaryTableExport  <- rbind(rep("",nColumns),modelSummaryTableExport) modelSummaryTableExport  <- rbind((c(modelEquation,rep("", nColumns-1))),modelSummaryTableExport) modelSummaryTableExport  <- rbind((c("Equation:",rep("",nColumns-1))),modelSummaryTableExport) indVariables <- paste(names(coef(Model.3)),collapse = ",")   #  extract list of independent variables from initial modelindVariables <- sub("(),","",indVariables) indVariables <- gsub("\\(","",indVariables) indVariables <- gsub("\\)","",indVariables) indVariables <- gsub("Intercept","",indVariables) modelSummaryTableExport  <- rbind((c(indVariables,rep("" , nColumns-1))),modelSummaryTableExport) modelSummaryTableExport  <- rbind((c("Independent Variables:",rep("",4),testMethod,rep("",nColumns-6))),modelSummaryTableExport) modelSummaryTableExport  <- rbind(c("Dependent Variable:",yName,rep("",3),variableSelection,rep("",nColumns - 6)),modelSummaryTableExport) modelSummaryTableExport  <- rbind(c("Logistic model:" ,modelName,rep("",3),"R script:"," GLOWdata.Model.3.04.29.22.23.44.r ",rep("",nColumns-7)),modelSummaryTableExport)write.table(modelSummaryTableExport, "clipboard-128", sep="\t", row.names=FALSE, col.names=FALSE)writeLines(" ")writeLines(" ")writeLines("***The output for this model has been written to the clipboard for importing to Excel.***")writeLines("Go back to your Excel file and hit the Import R button on the RegressIt menu to insert it on a new worksheet and add its statistics to the Model Summaries worksheet.")writeLines(" ");writeLines(" ")writeLines(" -----End script GLOWdata.Model.3.04.29.22.23.44.r produced by RegressItLogistic version 2022.12.14 on DESKTOP-F9IAE9K at time 04.29.22.23.44")writeLines("Various model inputs and outputs have been stored in global objects with generic names such as modelEquation, coeffMatrix, trainData (variables), trainTable (predictions, errors, etc.), testData and testTable (if any), and many others.")writeLines("They can all be deleted via the Clear Variables tool on the R code menu in RegressIt.")writeLines("Copy and paste the following line at the command prompt to re-run the script."); writeLines(" source('C:/Users/Jessica M/Desktop/GLOWdata.Model.3.04.29.22.23.44.r', encoding = 'UTF-8') ")
